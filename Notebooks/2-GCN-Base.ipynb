{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GCN-Model-2.ipynb","provenance":[{"file_id":"1tP5dw_7tj8Unjr493ngVFxwP-2NNFoOP","timestamp":1633086333880}],"collapsed_sections":[],"mount_file_id":"1MjNXJxTJEU9QjCduYomMc92hZPV4fokj","authorship_tag":"ABX9TyP4KlV4LmG6Hao/2j7TJ+lE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"klcSWpx2OaaF","executionInfo":{"status":"ok","timestamp":1633419936976,"user_tz":-120,"elapsed":197,"user":{"displayName":"Alvaro Mateos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14707240317005135342"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"3DaT68sbOnhK","executionInfo":{"status":"ok","timestamp":1633419941181,"user_tz":-120,"elapsed":3604,"user":{"displayName":"Alvaro Mateos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14707240317005135342"}}},"source":["#@title Encoders\n","import torch\n","import torch.nn as nn\n","from torch.nn import init\n","import torch.nn.functional as F\n","\n","class Encoder(nn.Module):\n","    \"\"\"\n","    Encodes a node's using 'convolutional' GraphSage approach\n","    \"\"\"\n","\n","    def __init__(self, features, feature_dim,\n","                 embed_dim, adj_lists, aggregator,\n","                 num_sample=10,\n","                 base_model=None, gcn=False, cuda=False,\n","                 kernel=\"GCN\",\n","                 feature_transform=False):\n","        super(Encoder, self).__init__()\n","\n","        self.features = features\n","        self.feat_dim = feature_dim\n","        self.adj_lists = adj_lists\n","        self.aggregator = aggregator\n","        self.num_sample = num_sample\n","        if base_model != None:\n","            self.base_model = base_model\n","\n","        self.gcn = gcn\n","        self.embed_dim = embed_dim\n","        self.cuda = cuda\n","        self.aggregator.cuda = cuda\n","        self.weight = nn.Parameter(\n","            torch.FloatTensor(embed_dim, self.feat_dim if self.gcn else 2 * self.feat_dim))\n","        init.xavier_uniform(self.weight)\n","\n","        self.kernel = kernel\n","\n","        num_mlp_layers = 2\n","        num_layers = 5\n","        self.num_layers = num_layers\n","\n","        input_dim = feature_dim\n","        hidden_dim = 512\n","        output_dim = embed_dim\n","        self.mlps = torch.nn.ModuleList()\n","        # List of batchnorms applied to the output of MLP (input of the final prediction linear layer)\n","        self.batch_norms = torch.nn.ModuleList()\n","\n","        for layer in range(self.num_layers-1):\n","            if layer == 0:\n","                self.mlps.append(\n","                    MLP(num_mlp_layers, input_dim, hidden_dim, hidden_dim))\n","            else:\n","                self.mlps.append(\n","                    MLP(num_mlp_layers, hidden_dim, hidden_dim, hidden_dim))\n","            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n","        self.mlps.append(\n","                    MLP(num_mlp_layers, hidden_dim, hidden_dim, embed_dim))\n","        self.batch_norms.append(nn.BatchNorm1d(embed_dim))\n","\n","        # Linear function that maps the hidden representation at dofferemt layers into a prediction score\n","        self.linears_prediction = torch.nn.ModuleList()\n","        for layer in range(num_layers):\n","            if layer == 0:\n","                self.linears_prediction.append(\n","                    nn.Linear(input_dim, output_dim))\n","            else:\n","                self.linears_prediction.append(\n","                    nn.Linear(hidden_dim, output_dim))\n","\n","    def forward(self, nodes):\n","        \"\"\"\n","        Generates embeddings for a batch of nodes.\n","\n","        nodes     -- list of nodes\n","        \"\"\"\n","        # print('encoder:', nodes)\n","        if self.kernel == \"GIN\":\n","            neigh_feats = self.aggregator.forward(nodes, [self.adj_lists[int(node)] for node in nodes],\n","                                                  self.num_sample, average=\"sum\")\n","            if self.cuda:\n","                self_feats = self.features(torch.LongTensor(nodes).cuda())\n","            else:\n","                self_feats = self.features(torch.LongTensor(nodes))\n","            h = torch.add(self_feats, neigh_feats)\n","            for layer in range(self.num_layers):\n","                pooled_rep = self.mlps[layer](h)\n","                h = self.batch_norms[layer](pooled_rep)\n","                h = F.relu(h)\n","            combined = h.t()\n","        else:\n","            neigh_feats = self.aggregator.forward(nodes, [self.adj_lists[int(node)] for node in nodes],\n","                                                  self.num_sample)\n","            if not self.gcn:\n","                if self.cuda:\n","                    self_feats = self.features(torch.LongTensor(nodes).cuda())\n","                else:\n","                    self_feats = self.features(torch.LongTensor(nodes))\n","                combined = torch.cat([self_feats, neigh_feats], dim=1)\n","            else:\n","                combined = neigh_feats\n","\n","            if self.kernel == \"GAT\":\n","                combined = F.elu(self.weight.mm(combined.t()))\n","            else:\n","                combined = F.relu(self.weight.mm(combined.t()))\n","        return combined\n"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"7K6f5OzfOnTu","executionInfo":{"status":"ok","timestamp":1633419941391,"user_tz":-120,"elapsed":219,"user":{"displayName":"Alvaro Mateos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14707240317005135342"}}},"source":["#@title Aggregators\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","\n","import random\n","\n","from torch.nn import init\n","\n","\"\"\"\n","Set of modules for aggregating embeddings of neighbors.\n","\"\"\"\n","\n","\n","class MeanAggregator(nn.Module):\n","    \"\"\"\n","    Aggregates a node's embeddings using mean of neighbors' embeddings\n","    \"\"\"\n","\n","    def __init__(self, features, features_dim=4096, cuda=False, gcn=False, kernel=\"GCN\"):\n","        \"\"\"\n","        Initializes the aggregator for a specific graph.\n","\n","        features -- function mapping LongTensor of node ids to FloatTensor of feature values.\n","        cuda -- whether to use GPU\n","        gcn --- whether to perform concatenation GraphSAGE-style, or add self-loops GCN-style\n","        \"\"\"\n","\n","        super(MeanAggregator, self).__init__()\n","\n","        self.features = features\n","        self.cuda = cuda\n","        self.gcn = gcn\n","        self.softmax = nn.Softmax(dim=1)\n","\n","        self.kernel = kernel\n","        self.attention = True if kernel == \"GAT\" else \"False\"\n","\n","        self.in_features = features_dim\n","        self.out_features = features_dim\n","\n","        self.weight = nn.Parameter(\n","            torch.FloatTensor(self.in_features, self.out_features, ))\n","        self.a = nn.Parameter(\n","            torch.FloatTensor(2 * self.out_features, 1))\n","        init.xavier_uniform(self.weight)\n","        init.xavier_uniform(self.a)\n","\n","        self.alpha = 0.2\n","        self.leakyrelu = nn.LeakyReLU(self.alpha)\n","\n","    def forward(self, nodes, to_neighs, num_sample=10, average=\"mean\"):\n","        \"\"\"\n","        nodes --- list of nodes in a batch\n","        to_neighs --- list of sets, each set is the set of neighbors for node in batch\n","        num_sample --- number of neighbors to sample. No sampling if None.\n","        \"\"\"\n","        # Local pointers to functions (speed hack)\n","        _set = set\n","        if not num_sample is None:\n","            _sample = random.sample\n","            samp_neighs = [_set(_sample(to_neigh,\n","                                        num_sample,\n","                                        )) if len(to_neigh) >= num_sample else to_neigh for to_neigh in to_neighs]\n","        else:\n","            samp_neighs = to_neighs\n","        if self.gcn:\n","            samp_neighs = [samp_neigh | set([nodes[i]])\n","                           for i, samp_neigh in enumerate(samp_neighs)]\n","\n","        unique_nodes_list = list(set.union(*samp_neighs))\n","        unique_nodes = {n: i for i, n in enumerate(unique_nodes_list)}\n","\n","        # print('agg:', nodes)\n","        # print('agg unique:', unique_nodes_list)\n","\n","        column_indices = [unique_nodes[n]\n","                          for samp_neigh in samp_neighs for n in samp_neigh]\n","        row_indices = [i for i in range(len(samp_neighs))\n","                       for j in range(len(samp_neighs[i]))]\n","\n","        mask = Variable(torch.zeros(len(samp_neighs), len(unique_nodes)))\n","        mask[row_indices, column_indices] = 1\n","\n","        attention_mask = Variable(torch.full(\n","            (len(samp_neighs), len(unique_nodes)), np.inf))\n","        attention_mask[row_indices, column_indices] = 0\n","\n","        zero_vec = -9e15 * torch.ones_like(mask)\n","\n","        if self.cuda:\n","            mask = mask.cuda()\n","            attention_mask.cuda()\n","\n","        num_neigh = mask.sum(1, keepdim=True)\n","        for ni, num in enumerate(num_neigh):\n","            if num == 0:\n","                num_neigh[ni] = 1\n","\n","        if self.cuda:\n","            embed_matrix = self.features(\n","                torch.LongTensor(unique_nodes_list).cuda())\n","            feature_matrix = self.features(torch.LongTensor(nodes).cuda())\n","        else:\n","            embed_matrix = self.features(torch.LongTensor(unique_nodes_list))\n","            feature_matrix = self.features(torch.LongTensor(nodes))\n","\n","        if self.kernel == \"GAT\":\n","            # attention_matrix = feature_matrix.mm(embed_matrix.t())\n","            #             # attention_matrix = attention_matrix.mul(mask)\n","            #             # attention_matrix = attention_matrix - attention_mask\n","            #             # attention = self.softmax(attention_matrix)\n","            # mask = mask.mul(attention)\n","            feature_matrix = torch.mm(feature_matrix, self.weight)\n","            embed_matrix = torch.mm(embed_matrix, self.weight)\n","            N = feature_matrix.size()[0]\n","            M = embed_matrix.size()[0]\n","\n","            a_input = torch.cat([feature_matrix.repeat(1, M).view(N * M, -1), embed_matrix.repeat(N, 1)],\n","                                dim=1).view(N, -1, 2 * self.out_features)\n","            attention_matrix = self.leakyrelu(\n","                torch.matmul(a_input, self.a).squeeze(2))\n","            # print(attention_matrix.size())\n","            # print(mask.size())\n","\n","            # attention_matrix = feature_matrix.mm(embed_matrix.t())\n","\n","            attention = torch.where(mask > 0, attention_matrix, zero_vec)\n","            attention = self.softmax(attention)\n","            # mask = mask.mul(attention)\n","            to_feats = attention.mm(embed_matrix)\n","        elif self.kernel == \"GCN\":\n","            if average == \"mean\":\n","                mask = mask.div(num_neigh)\n","            to_feats = mask.mm(embed_matrix)\n","        elif self.kernel == \"GIN\":\n","            to_feats = mask.mm(embed_matrix)\n","\n","        return to_feats\n","\n","\n","class AttentionAggregator(nn.Module):\n","    \"\"\"\n","    Aggregates a node's embeddings with attention\n","    \"\"\"\n","\n","    def __init__(self, features, in_features=4096, out_features=1024, cuda=False, gcn=False, attention_dim=512):\n","        \"\"\"\n","        Initializes the aggregator for a specific graph.\n","\n","        features -- function mapping LongTensor of node ids to FloatTensor of feature values.\n","        cuda -- whether to use GPU\n","        gcn --- whether to perform concatenation GraphSAGE-style, or add self-loops GCN-style\n","        \"\"\"\n","\n","        super(AttentionAggregator, self).__init__()\n","\n","        self.features = features\n","        self.cuda = cuda\n","        self.gcn = gcn\n","        self.softmax = nn.Softmax(dim=1)\n","        self.attention = True\n","\n","        self.in_features = in_features\n","        self.out_features = out_features\n","\n","        self.weight = nn.Parameter(\n","            torch.FloatTensor(out_features, in_features))\n","\n","        self.a = nn.Parameter(\n","            torch.FloatTensor(2 * out_features, 1))\n","\n","    def forward(self, nodes, to_neighs, num_sample=10):\n","        \"\"\"\n","        nodes --- list of nodes in a batch\n","        to_neighs --- list of sets, each set is the set of neighbors for node in batch\n","        num_sample --- number of neighbors to sample. No sampling if None.\n","        \"\"\"\n","        # Local pointers to functions (speed hack)\n","        _set = set\n","        if not num_sample is None:\n","            _sample = random.sample\n","            samp_neighs = [_set(_sample(to_neigh,\n","                                        num_sample,\n","                                        )) if len(to_neigh) >= num_sample else to_neigh for to_neigh in to_neighs]\n","        else:\n","            samp_neighs = to_neighs\n","        if self.gcn:\n","            samp_neighs = [samp_neigh | set([nodes[i]])\n","                           for i, samp_neigh in enumerate(samp_neighs)]\n","\n","        unique_nodes_list = list(set.union(*samp_neighs))\n","        unique_nodes = {n: i for i, n in enumerate(unique_nodes_list)}\n","\n","        column_indices = [unique_nodes[n]\n","                          for samp_neigh in samp_neighs for n in samp_neigh]\n","        row_indices = [i for i in range(len(samp_neighs))\n","                       for j in range(len(samp_neighs[i]))]\n","\n","        mask = Variable(torch.zeros(len(samp_neighs), len(unique_nodes)))\n","        mask[row_indices, column_indices] = 1\n","\n","        zero_vec = -9e15 * torch.ones_like(mask)\n","\n","        if self.cuda:\n","            mask = mask.cuda()\n","\n","        num_neigh = mask.sum(1, keepdim=True)\n","        for ni, num in enumerate(num_neigh):\n","            if num == 0:\n","                num_neigh[ni] = 1\n","\n","        if self.cuda:\n","            embed_matrix = self.features(\n","                torch.LongTensor(unique_nodes_list).cuda())\n","            feature_matrix = self.features(torch.LongTensor(nodes).cuda())\n","        else:\n","            embed_matrix = self.features(torch.LongTensor(unique_nodes_list))\n","            feature_matrix = self.features(torch.LongTensor(nodes))\n","\n","        attention_matrix = feature_matrix.mm(embed_matrix.t())\n","        attention = torch.where(mask > 0, attention_matrix, zero_vec)\n","        attention = self.softmax(attention)\n","\n","        to_feats = attention.mm(embed_matrix)\n","        return to_feats\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"4AdnlfLjOnrU","executionInfo":{"status":"ok","timestamp":1633419941393,"user_tz":-120,"elapsed":6,"user":{"displayName":"Alvaro Mateos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14707240317005135342"}}},"source":["#@title MLP\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","###MLP with lienar output\n","class MLP(nn.Module):\n","    def __init__(self, num_layers, input_dim, hidden_dim, output_dim):\n","        '''\n","            num_layers: number of layers in the neural networks (EXCLUDING the input layer). If num_layers=1, this reduces to linear model.\n","            input_dim: dimensionality of input features\n","            hidden_dim: dimensionality of hidden units at ALL layers\n","            output_dim: number of classes for prediction\n","            device: which device to use\n","        '''\n","    \n","        super(MLP, self).__init__()\n","\n","        self.linear_or_not = True #default is linear model\n","        self.num_layers = num_layers\n","\n","        if num_layers < 1:\n","            raise ValueError(\"number of layers should be positive!\")\n","        elif num_layers == 1:\n","            #Linear model\n","            self.linear = nn.Linear(input_dim, output_dim)\n","        else:\n","            #Multi-layer model\n","            self.linear_or_not = False\n","            self.linears = torch.nn.ModuleList()\n","            self.batch_norms = torch.nn.ModuleList()\n","        \n","            self.linears.append(nn.Linear(input_dim, hidden_dim))\n","            for layer in range(num_layers - 2):\n","                self.linears.append(nn.Linear(hidden_dim, hidden_dim))\n","            self.linears.append(nn.Linear(hidden_dim, output_dim))\n","\n","            for layer in range(num_layers - 1):\n","                self.batch_norms.append(nn.BatchNorm1d((hidden_dim)))\n","\n","    def forward(self, x):\n","        if self.linear_or_not:\n","            #If linear model\n","            return self.linear(x)\n","        else:\n","            #If MLP\n","            h = x\n","            for layer in range(self.num_layers - 1):\n","                h = F.relu(self.batch_norms[layer](self.linears[layer](h)))\n","            return self.linears[self.num_layers - 1](h)\n","\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"LrhvTlvwOnxJ","executionInfo":{"status":"ok","timestamp":1633419942786,"user_tz":-120,"elapsed":1398,"user":{"displayName":"Alvaro Mateos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14707240317005135342"}}},"source":["#@title Model Multi\n","import functools\n","\n","import sklearn\n","import torch\n","import torch.nn as nn\n","from torch.nn import init\n","from torch.autograd import Variable\n","\n","import numpy as np\n","import time\n","import random\n","\n","import warnings\n","import sklearn.exceptions\n","\n","\n","warnings.filterwarnings(\n","    \"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n","\n","\n","def log(func):\n","    @functools.wraps(func)\n","    def wrapper(*args, **kw):\n","        start_time = time.time()\n","        res = func(*args, **kw)\n","        end_time = time.time()\n","        print('%s executed in %ss' % (func.__name__, end_time - start_time))\n","        return res\n","\n","    return wrapper\n","\n","\n","class DiseasesClassifier(nn.Module):\n","\n","    def __init__(self, num_classes, enc):\n","        super(DiseasesClassifier, self).__init__()\n","        self.enc = enc\n","\n","        # self.xent = nn.CrossEntropyLoss()\n","        self.xent = nn.BCEWithLogitsLoss()\n","\n","        self.weight = nn.Parameter(\n","            torch.FloatTensor(num_classes, enc.embed_dim))\n","        init.xavier_uniform(self.weight)\n","\n","        self.a = nn.Linear(enc.embed_dim, 1)\n","\n","    @staticmethod\n","    def binary_loss(y_pred, y):\n","        y_pred = torch.from_numpy(y_pred)\n","        logits = (y * y_pred.clamp(1e-12).log() + (1 - y)\n","                  * (1 - y_pred).clamp(1e-12).log()).mean()\n","        return -logits\n","\n","    def forward(self, nodes):\n","        embeds = self.enc(nodes)\n","        scores = self.weight.mm(embeds)\n","        return scores.t()\n","\n","    def loss(self, nodes, labels):\n","        scores = self.forward(nodes)\n","        return self.xent(scores, labels)\n","\n","    def forward_hinge(self, nodes):\n","        embeds = self.enc(nodes)\n","        return self.a(embeds.t())  # a = nn.Linear(enc.embed_dim, 1)\n","\n","    def hinge_loss(self, nodes, labels):\n","        h_loss = self.forward_hinge(nodes)\n","        return torch.mean(torch.clamp(1 - h_loss * labels, min=0))\n","\n","\n","def evaluate(data_name, val_output, test_labels, val, topk=(1, 2, 3, 4, 5,)):\n","    print(\"----\" * 25)\n","    print()\n","    print(\"%s: \" % data_name)\n","\n","    # shape: batchnum * classnum\n","    target = torch.LongTensor(test_labels[val])\n","    output = val_output  # shape: batchnum * classnum\n","\n","    # print(target.shape, output.shape)\n","    maxk = max(topk)\n","    batch_size = target.size(0)\n","\n","    _, pred = output.topk(maxk, 1, True, True)\n","    # print(pred)\n","    correct = torch.zeros_like(pred)\n","    for i in range(batch_size):\n","        for k in range(maxk):\n","            correct[i, k] = 1 if target[i][pred[i, k]] == 1 else 0\n","    correct = correct.t()\n","\n","    correct_target = target.sum(1, keepdim=True).squeeze().float()\n","\n","    for k in topk:\n","        correct_k = correct[:k].sum(0, keepdim=True).squeeze().float()\n","\n","        precision_k = 0.0\n","        # recall_k = 0.0\n","        for i in range(0, batch_size):\n","            # _k = k if k < correct_target[i].data else correct_target[i]\n","            _k = k\n","            precision_k += correct_k[i] / _k\n","            # recall_k += correct_k[i] / correct_target[i]\n","        precision_k = precision_k / batch_size\n","        # recall_k = recall_k / batch_size\n","\n","        # print(\"precision @\", k, precision_k.data)\n","        # print(\"recall @\", k, recall_k.data)\n","\n","        # precision_k = correct_k / k\n","        # precision_k = precision_k.sum() / batch_size\n","\n","        recall_k = correct_k / correct_target\n","        recall_k = recall_k.sum() / batch_size\n","\n","        f1_k = 2 * precision_k * recall_k / (precision_k + recall_k)\n","\n","        print(\"precision @ %d : %.5f, recall @ %d : %.5f, f1 @ %d : %.5f\" % (\n","            k, precision_k.data, k, recall_k.data, k, f1_k.data))\n","        # print(\"precision @\", k, precision_k.data)\n","        # print(\"recall @\", k, recall_k.data)\n","        # print(\"f1 @\", k, f1_k.data)\n","\n","        # print(\"precision@%d: %f\" & (k, precision_k))\n","        # print(\"recall@%d: %f\" & (k, recall_k))\n","    print()\n","\n","\n","class DiseasesPredictor:\n","    def __init__(self, feat_data, b_labels, multi_class_num, labels, adj_lists, feature_dim,\n","                 train_enc_num, train_enc_dim, train_sample_num,\n","                 train, test,\n","                 kernel='gcn',  topk=(1, 2, 3, 4, 5,),\n","                 weights_flag=False, weights=[0.5, 0.5],\n","                 gcn=False, agg_gcn=True, cuda=False):\n","\n","        self.cuda = cuda\n","        self.gcn = gcn\n","        self.agg_gcn = agg_gcn\n","\n","        self.train_original = train.copy()\n","        self.test_original = test.copy()\n","\n","        self.train = train\n","        self.test = test\n","        self.test_rare = [i for i in np.where(\n","            (b_labels > 0))[0].squeeze() if i in self.test]\n","        self.test_rare_index = [self.test.index(i) for i in self.test_rare]\n","\n","        self.b_labels = b_labels\n","        self.labels = labels\n","\n","        self.bi_class_num = 2\n","        self.multi_class_num = multi_class_num\n","\n","        # nodes' features (random setting)\n","        self.features = nn.Embedding(len(feat_data), feature_dim)\n","        self.features.weight = nn.Parameter(\n","            torch.FloatTensor(feat_data), requires_grad=False)\n","        self.adj_lists = adj_lists  # edges' information\n","\n","        # model parameters\n","        self.train_enc_dim = train_enc_dim\n","        self.train_enc_num = train_enc_num\n","        self.kernel = kernel\n","        self.attention = True if kernel == \"GAT\" else False\n","        self.feature_dim = feature_dim\n","        self.train_sample_num = train_sample_num\n","\n","        self.topk = topk\n","\n","        # weighted cross-entropy\n","        self.weights_flag = weights_flag\n","        self.class_weights = torch.FloatTensor(weights)\n","\n","        # labels for test\n","        self.test_b_labels = b_labels\n","        self.test_adj = adj_lists\n","\n","        # inductive settings\n","        self.is_inductive = False\n","        self.test_sample_num = train_sample_num\n","        self.test_features = self.features\n","\n","        # build aggregator and encoders\n","        # default: transductive setting\n","\n","        self.agg1 = MeanAggregator(self.features, features_dim=feature_dim,\n","                                   cuda=self.cuda, kernel=self.kernel, gcn=self.agg_gcn)\n","        self.enc1 = Encoder(self.features, feature_dim, train_enc_dim[0], adj_lists, self.agg1,\n","                            gcn=self.gcn, cuda=self.cuda, kernel=self.kernel)\n","\n","        self.agg2 = MeanAggregator(lambda nodes: self.enc1(nodes).t(), features_dim=self.enc1.embed_dim,\n","                                   cuda=self.cuda, kernel=self.kernel, gcn=self.agg_gcn)\n","        self.enc2 = Encoder(lambda nodes: self.enc1(nodes).t(), self.enc1.embed_dim, train_enc_dim[1], adj_lists,\n","                            self.agg2, base_model=self.enc1, gcn=self.gcn, cuda=self.cuda, kernel=self.kernel)\n","\n","        self.agg3 = MeanAggregator(lambda nodes: self.enc2(nodes).t(), features_dim=self.enc2.embed_dim,\n","                                   cuda=self.cuda, kernel=self.kernel, gcn=self.agg_gcn)\n","        self.enc3 = Encoder(lambda nodes: self.enc2(nodes).t(), self.enc2.embed_dim, train_enc_dim[2], adj_lists,\n","                            self.agg3, base_model=self.enc2, gcn=self.gcn, cuda=self.cuda, kernel=self.kernel)\n","        self.agg4 = MeanAggregator(lambda nodes: self.enc3(nodes).t(), features_dim=self.enc3.embed_dim,\n","                                   cuda=self.cuda, kernel=self.kernel, gcn=self.agg_gcn)\n","        self.enc4 = Encoder(lambda nodes: self.enc3(nodes).t(), self.enc3.embed_dim, train_enc_dim[3], adj_lists,\n","                            self.agg4, base_model=self.enc3, gcn=self.gcn, cuda=self.cuda, kernel=self.kernel)\n","        self.enc1.num_samples = self.train_sample_num[0]\n","        self.enc2.num_samples = self.train_sample_num[1]\n","        self.enc3.num_samples = self.train_sample_num[2]\n","        self.enc4.num_samples = self.train_sample_num[3]\n","\n","    def set_classifier(self, class_num, train_enc_num):\n","        classifier = DiseasesClassifier(class_num, self.enc2)\n","        if train_enc_num == 1:\n","            classifier = DiseasesClassifier(class_num, self.enc1)\n","        elif train_enc_num == 2:\n","            classifier = DiseasesClassifier(class_num, self.enc2)\n","        elif train_enc_num == 3:\n","            classifier = DiseasesClassifier(class_num, self.enc3)\n","        elif train_enc_num == 4:\n","            classifier = DiseasesClassifier(class_num, self.enc4)\n","        return classifier\n","\n","    def run(self, loop_num, batch_num, lr):\n","        if loop_num is None:\n","            loop_num = [100, 500]\n","\n","        multi_classifier = self.set_classifier(\n","            class_num=self.multi_class_num, train_enc_num=self.train_enc_num)\n","\n","        self.__train__(multi_classifier, train=self.train, labels=self.labels,\n","                       loop_num=loop_num, batch_num=batch_num, lr=lr)\n","\n","        multi_result_direct = multi_classifier.forward(self.test)\n","\n","        evaluate(\"multi classification (overall)\",\n","                 multi_result_direct,\n","                 self.labels,\n","                 self.test,\n","                 topk=self.topk)\n","        print(\"len of rare:\", len(self.test_rare_index))\n","        evaluate(\"multi classification (rare)\",\n","                 multi_result_direct[self.test_rare_index],\n","                 self.labels,\n","                 self.test_rare,\n","                 topk=self.topk)\n","\n","    def __train__(self, selected_model, train, labels, loop_num=100, batch_num=512, lr=0.01):\n","        np.random.seed(1)\n","        random.seed(1)\n","\n","        optimizer = torch.optim.SGD(\n","            filter(lambda p: p.requires_grad, selected_model.parameters()), lr=lr)\n","        # optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, selected_model.parameters()), lr=lr, betas=(0.9, 0.99))\n","        times = []\n","\n","        for batch in range(loop_num):\n","            batch_nodes = train[:batch_num]\n","            random.shuffle(train)\n","            start_time = time.time()\n","            optimizer.zero_grad()\n","            loss = selected_model.loss(batch_nodes,\n","                                       Variable(torch.FloatTensor(labels[np.array(batch_nodes, dtype=np.int64)])))\n","            loss.backward()\n","            optimizer.step()\n","            end_time = time.time()\n","            times.append(end_time - start_time)\n","            if batch % 100 == 0:\n","              print(batch, loss.data)\n","\n","        print()\n","        print(\"Average batch time:\", np.mean(times))\n","        print()\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"vDEzlGLlOnuN","executionInfo":{"status":"ok","timestamp":1633419942794,"user_tz":-120,"elapsed":16,"user":{"displayName":"Alvaro Mateos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14707240317005135342"}}},"source":["#@title Model\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"UCGfpH_LOnzr","executionInfo":{"status":"ok","timestamp":1633419942795,"user_tz":-120,"elapsed":16,"user":{"displayName":"Alvaro Mateos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14707240317005135342"}}},"source":["#@title Unsupervised\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g0s6S0r_RBlf","executionInfo":{"status":"ok","timestamp":1633423235991,"user_tz":-120,"elapsed":3293210,"user":{"displayName":"Alvaro Mateos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14707240317005135342"}},"outputId":"fb7ea38d-e58a-4236-ab73-1abda888c8f0"},"source":["import time\n","import pickle\n","import numpy as np\n","\n","file_path = \"/content/drive/MyDrive/GCNModel/sample_data2/sample_graph\"\n","node_list = pickle.load(open(file_path + \".nodes.pkl\", \"rb\"))\n","adj_lists = pickle.load(open(file_path + \".adj.pkl\", \"rb\"))\n","rare_patient = pickle.load(open(file_path + \".rare.label.pkl\", \"rb\"))\n","labels = pickle.load(open(file_path + \".label.pkl\", \"rb\"))\n","node_map = pickle.load(open(file_path + \".map.pkl\", \"rb\"))\n","train = pickle.load(open(file_path + \".train.pkl\", \"rb\"))\n","test = pickle.load(open(file_path + \".test.pkl\", \"rb\"))\n","\n","multi_class_num = 101\n","feature_dim = 10000\n","epoch = 8000\n","batch_num = 200\n","lr = 0.3\n","feat_data = np.random.random((50000, feature_dim))\n","train_enc_dim = [1000, 1000, 1000, 1000]\n","t1 = time.time()\n","model = DiseasesPredictor(feat_data=feat_data,\n","                          b_labels=rare_patient,\n","                          multi_class_num=multi_class_num,\n","                          labels=labels,\n","                          adj_lists=adj_lists,\n","                          feature_dim=feature_dim,\n","                          train_enc_num=1,\n","                          train_enc_dim=train_enc_dim,\n","                          train_sample_num=[5, 5, 5, 5],\n","                          train=train, test=test,\n","                          kernel='GCN',\n","                          topk=(1, 2, 3, 4, 5,))\n","\n","model.run(epoch, batch_num, lr)  # epoch, batch_num, lr\n","print(feature_dim, train_enc_dim)\n","print(\"running time:\", time.time()-t1, \"s\")\n"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:49: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"]},{"output_type":"stream","name":"stdout","text":["0 tensor(0.7340)\n","100 tensor(0.0485)\n","200 tensor(0.0412)\n","300 tensor(0.0342)\n","400 tensor(0.0271)\n","500 tensor(0.0214)\n","600 tensor(0.0167)\n","700 tensor(0.0130)\n","800 tensor(0.0105)\n","900 tensor(0.0086)\n","1000 tensor(0.0072)\n","1100 tensor(0.0062)\n","1200 tensor(0.0053)\n","1300 tensor(0.0047)\n","1400 tensor(0.0041)\n","1500 tensor(0.0037)\n","1600 tensor(0.0034)\n","1700 tensor(0.0030)\n","1800 tensor(0.0028)\n","1900 tensor(0.0026)\n","2000 tensor(0.0024)\n","2100 tensor(0.0022)\n","2200 tensor(0.0021)\n","2300 tensor(0.0019)\n","2400 tensor(0.0018)\n","2500 tensor(0.0017)\n","2600 tensor(0.0016)\n","2700 tensor(0.0015)\n","2800 tensor(0.0015)\n","2900 tensor(0.0014)\n","3000 tensor(0.0013)\n","3100 tensor(0.0013)\n","3200 tensor(0.0012)\n","3300 tensor(0.0012)\n","3400 tensor(0.0011)\n","3500 tensor(0.0011)\n","3600 tensor(0.0010)\n","3700 tensor(0.0010)\n","3800 tensor(0.0010)\n","3900 tensor(0.0009)\n","4000 tensor(0.0009)\n","4100 tensor(0.0009)\n","4200 tensor(0.0009)\n","4300 tensor(0.0008)\n","4400 tensor(0.0008)\n","4500 tensor(0.0008)\n","4600 tensor(0.0008)\n","4700 tensor(0.0007)\n","4800 tensor(0.0007)\n","4900 tensor(0.0007)\n","5000 tensor(0.0007)\n","5100 tensor(0.0007)\n","5200 tensor(0.0006)\n","5300 tensor(0.0006)\n","5400 tensor(0.0006)\n","5500 tensor(0.0006)\n","5600 tensor(0.0006)\n","5700 tensor(0.0006)\n","5800 tensor(0.0006)\n","5900 tensor(0.0006)\n","6000 tensor(0.0005)\n","6100 tensor(0.0005)\n","6200 tensor(0.0005)\n","6300 tensor(0.0005)\n","6400 tensor(0.0005)\n","6500 tensor(0.0005)\n","6600 tensor(0.0005)\n","6700 tensor(0.0005)\n","6800 tensor(0.0005)\n","6900 tensor(0.0005)\n","7000 tensor(0.0004)\n","7100 tensor(0.0004)\n","7200 tensor(0.0004)\n","7300 tensor(0.0004)\n","7400 tensor(0.0004)\n","7500 tensor(0.0004)\n","7600 tensor(0.0004)\n","7700 tensor(0.0004)\n","7800 tensor(0.0004)\n","7900 tensor(0.0004)\n","\n","Average batch time: 0.409729882389307\n","\n","----------------------------------------------------------------------------------------------------\n","\n","multi classification (overall): \n","precision @ 1 : 0.54000, recall @ 1 : 0.54000, f1 @ 1 : 0.54000\n","precision @ 2 : 0.31500, recall @ 2 : 0.63000, f1 @ 2 : 0.42000\n","precision @ 3 : 0.22333, recall @ 3 : 0.67000, f1 @ 3 : 0.33500\n","precision @ 4 : 0.17250, recall @ 4 : 0.69000, f1 @ 4 : 0.27600\n","precision @ 5 : 0.14000, recall @ 5 : 0.70000, f1 @ 5 : 0.23333\n","\n","len of rare: 100\n","----------------------------------------------------------------------------------------------------\n","\n","multi classification (rare): \n","precision @ 1 : 0.54000, recall @ 1 : 0.54000, f1 @ 1 : 0.54000\n","precision @ 2 : 0.31500, recall @ 2 : 0.63000, f1 @ 2 : 0.42000\n","precision @ 3 : 0.22333, recall @ 3 : 0.67000, f1 @ 3 : 0.33500\n","precision @ 4 : 0.17250, recall @ 4 : 0.69000, f1 @ 4 : 0.27600\n","precision @ 5 : 0.14000, recall @ 5 : 0.70000, f1 @ 5 : 0.23333\n","\n","10000 [1000, 1000, 1000, 1000]\n","running time: 3287.7890152931213 s\n"]}]},{"cell_type":"code","metadata":{"id":"TEPl3e-URB5u","executionInfo":{"status":"ok","timestamp":1633423235993,"user_tz":-120,"elapsed":16,"user":{"displayName":"Alvaro Mateos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14707240317005135342"}}},"source":[""],"execution_count":7,"outputs":[]}]}